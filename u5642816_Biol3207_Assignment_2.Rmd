---
title: "BIOL3207 Assignment 2"
author: "Kalon Peters, u5642816"
date: "31 October 2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE, warning = FALSE}
# load libraries
library(tidyverse)
library(metafor)
library(orchaRd)
library(flextable)
```

[My GitHub Repository](https://github.com/kpet0008/u5642816_Biol3207_Assignment_2)

### Q1
Correct analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv) to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.
```{r}
# Load the Clark et al. (2020) data file from the data folder into an object "Clark"
path <- "./data/OA_activitydat_20190302_BIOL3207.csv"
Clark <- read_csv(path)
```

```{r}
# Clean the data

# Removing missing data and turning character variables into factors
Clark_clean <- Clark %>% filter(!is.na(activity)) %>% filter(!is.na(animal_id)) %>% mutate(across(where(is.character), factor))

# Check spelling in factor variables we care about
# If no entries are misspelled, there should be 6 levels to species, and 2 to treatment
str(Clark_clean)

```

```{r}
# Create a new object which contains the summary values we're interested in
#  ie. for each species and treatment, the mean activity, SD activity, and n

# I recognise this is not the tidyest way to do this, however it matches the format of the metadata data set

Clark_summary <- bind_cols(
  unique(Clark_clean %>% group_by(species) %>% filter(treatment == "control") 
         %>% mutate(ctrl.mean = mean(activity)) 
         %>% mutate(ctrl.sd = sd(activity)) 
         %>% select(species, ctrl.mean, ctrl.sd) 
         %>% arrange(species)), 
  (ctrl.n = (Clark_clean %>% group_by(species) %>% filter(treatment == "control") 
         %>% arrange(species)
         %>% count())[,2]), 
  unique(Clark_clean %>% group_by(species) %>% filter(treatment == "CO2") 
         %>% mutate(oa.mean = mean(activity)) 
         %>% mutate(oa.sd = sd(activity)) 
         %>% select(species, oa.mean, oa.sd) 
         %>% arrange(species))[,2:3], 
  (oa.n = (Clark_clean %>% group_by(species) %>% filter(treatment == "CO2") 
         %>% arrange(species) 
         %>% count())[,2])
  )

# rename the n columns and species column, to match the broader metadata data set used later
Clark_summary <- Clark_summary %>% rename(ctrl.n = n...4, oa.n = n...7, Species = species)

# replace the abbreviated species names with their full scientific names, to match the broader metadata data set used later
# Species name: acantho = Acanthochromis polyacanthus; 
#               ambon = Pomacentrus amboinensis; 
#               chromis = Chromis atripectoralis; 
#               humbug = Dascyllus aruanus; 
#               lemon = Pomacentrus moluccensis	
#               whitedams = Dischistodus perspicillatus
levels(Clark_summary$Species) <- c('Acanthochromis polyacanthus', 'Pomacentrus amboinensis', 'Chromis atripectoralis', 'Dascyllus aruanus', 'Pomacentrus moluccensis', 'Dischistodus perspicillatus')

# convert the species factor back to a character, to match the broader metadata data set used later
# why did I make species a factor in the first place? Because it makes renaming it much cleaner, and to make it easier to find any possible spelling errors in the data.

Clark_summary <- mutate(Clark_summary, Species = as.character(Species))
```


### Q2
Through coding, merge the summary statistics generated from 1) with the metadata (i.e., clark_paper_data.csv) from Clark et al. (2020).
```{r}
# Load the Clark et al. (2020) metadata from the data folder into an object "Clark_meta"
path <- "./data/clark_paper_data.csv"
Clark_meta <- read_csv(path)
```

```{r}
# merge the summary statistics with the metadata
Clark_meta <- bind_cols(Clark_meta, Clark_summary)
```

### Q3
Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset (i.e., ocean_meta_data.csv).
```{r}
# Load the larger meta-analysis dataset from the data folder into an object "meta"
path <- "./data/ocean_meta_data.csv"
meta <- read_csv(path)
```

```{r}
# A few columns need to be turned into character variables
# Additionally, NA values in the "Cue/stimulus type" column should be replaced with '-' to match the rest of the data set.
Clark_meta <- Clark_meta %>% mutate(`Pub year IF` = as.character(`Pub year IF`)) %>% mutate(`2017 IF`= as.character(`2017 IF`)) %>% mutate(`Env cue/stimulus?` = as.character(`Env cue/stimulus?`)) %>% mutate(`Cue/stimulus type` = as.character(`Cue/stimulus type`)) %>% mutate(`Cue/stimulus type` = replace_na(`Cue/stimulus type`, "-"))

str(Clark_meta)

```

```{r}
# merge the combined summary statistics and metadata from Clark et al. (2020) into the larger meta-analysis dataset

meta <- bind_rows(meta, Clark_meta)
```

### Q4
Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function.
```{r}
# lnRR produces NaNs whenever the sign of ctrl.mean and oa.mean are different.
# Since these either break or are automatically excluded from future analyses, and there arent many rows like this, its easiest to just remove them now
lnrr <- meta %>% filter(ctrl.mean > 0 & oa.mean > 0 | ctrl.mean < 0 & oa.mean < 0)
lnrr <- escalc(measure="ROM", m1i = ctrl.mean, m2i = oa.mean, sd1i = ctrl.sd, sd2i = oa.sd, n1i = ctrl.n, n2i = oa.n, data=lnrr)

```

### Q5
Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.
```{r}
# Producing an rma.mv model using the effect size (yi) and sampling variance (vi) calculated above
# Authors and Study are invluded as random effects

# Because the ratio of largest to smallest sampling variance extremely large, the model has a very difficult time producing a result with default settings.
# By default, rel.tol = 1e-8. To make this model actually produce a result, I've had to increase it all the way to 1e-2
# In theory I should also be able to make the model run more iterations with iter.max, to maybe get a result without changing the tolerance,
#  but for some reason that's not actually making it run more iterations in practice.
MLMA <- metafor::rma.mv(yi ~ 1, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr,
                   control = list(rel.tol = 1e-2))
MLMA
```

```{r}
# remove this before submitting
coef(MLMA)
MLMA$ci.lb
MLMA$ci.ub
MLMA$pval
```



### Q6
Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include:
  Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).

```{}
The effect size is not significantly different from 0
```

  Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor)
```{r}
predict(MLMA)

## Calculate I2
i2_vals <- orchaRd::i2_ml(MLMA)

## Make a pretty table. First, lets clean up the names of the different I2 estimates. Lets remove I2_. It's a string, so, we can use some regular expressions to fix that. `gsub` is pretty useful. You put a pattern in and tell it what you would like to replace the text with. In this case, just blank will do! Then, we'll make the first letter of what is left capitalised.
i2 <- tibble(type = firstup(gsub("I2_", "",names(i2_vals))), I2 = i2_vals)


# You remember flextable. Now, lets make a pretty table. We can so some nice modifications here too.

flextable(i2) %>% 
    align(part = "header", align = "center") %>% 
  compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>% 
  compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")), as_b("(%)")))
```
 




### Q7
Funnel plot for visually assessing the possibility of publication bias.
```{r, funnel, fig.align='center', fig.cap= "Funnel plot depicting precision (1 / SE) against the log response ratio (lnRR)."}
# Due to the extreme variation in the size of sampling error (and subsequently, the inverse standard error),
#  I have log transformed the y axis to compress the y axis to a readable scale.
# In order to do this, it seemed to be necessary to use ggplot rather than metafor to create the plot.
# As a consequence, the plot cant be contour-enhanced.

ggplot(lnrr, aes(y = abs(log(1/sqrt(vi))), x = yi)) + geom_point() + geom_vline(aes(xintercept = 0)) +
    labs(y = "Precision (1/SE)", x = "log Reponse Ratio (lnRR)") + theme_bw()
```



### Q8
Time-lag plot assessing how effect sizes may or may not have changed through time.
```{r, time-lag, fig.align='center', fig.cap= "Time-lag plot depicting effect size (lnRR) against the year in which the paper was published. Point size is scaled according to precision, 1/sqrt(vi)"}
# 'Year..online.' is the year the study was published, on the x axis
# 'yi' in the log response ratio, on the y axis
# Points are scaled in size according to their inverse standard error, 1/sqrt(vi)
# A linear model trend line is also applied
ggplot(lnrr, aes(x=Year..online., y=yi, size=1/sqrt(vi))) + geom_point(alpha=0.5) + geom_smooth(method = "lm", show.legend = F) + labs(size="Precision (1/SE)") + ylab("Effect Size (lnRR)") + xlab("Year Published")
```

### Q9
Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
# centre the year on zero and remove the rows with an NA value since apparently they break the meta regression
lnrr_temp <-  na.omit(lnrr) %>% mutate(Year_c = mean(Year..online.) - Year..online.)

# As before, rel.tol needs to be greatly increased compared to the default setting for this model to run
metareg_time_c <- rma.mv(yi ~ 1 + Year_c, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr_temp,
                   control = list(rel.tol = 1e-1))
summary(metareg_time_c) 
```
test for residual heterogeneity tells us there is a lot of unexplained variation left over
Test of moderators tells us that the moderators (in this case, year) are not explaining a significant amount of variation



### Q10
Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases
```{r}
# I see we've been asked to use inverse sampling variance as a moderator.
# I have used sampling variance instead, because the model simply does not run with inverse sampling variance,
#  and sampling variance is what was used in week 10 to test for file-drawer bias.
# I can only hope that asking for inverse sampling variance was a mistake.

metareg_time_c <- rma.mv(yi ~ vi, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr_temp,
                   control = list(rel.tol = 1e-2))
summary(metareg_time_c) 
```


### Q11
A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

### Q12
Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?








