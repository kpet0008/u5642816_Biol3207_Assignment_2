---
title: "BIOL3207 Assignment 2"
author: "Kalon Peters, u5642816"
date: "31 October 2022"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE, warning = FALSE}
# load libraries
library(tidyverse)
library(metafor)
library(orchaRd)
library(flextable)
```

[My GitHub Repository](https://github.com/kpet0008/u5642816_Biol3207_Assignment_2)

### Q1
Correct analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv) to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.
```{r}
# Load the Clark et al. (2020) data file from the data folder into an object "Clark"
path <- "./data/OA_activitydat_20190302_BIOL3207.csv"
Clark <- read_csv(path)
```

```{r}
# Clean the data

# Removing missing data and turning character variables into factors
Clark_clean <- Clark %>% filter(!is.na(activity)) %>% filter(!is.na(animal_id)) %>% mutate(across(where(is.character), factor))

# Check spelling in factor variables we care about
# If no entries are misspelled, there should be 6 levels to species, and 2 to treatment
str(Clark_clean)

```

```{r}
# Create a new object which contains the summary values we're interested in
#  ie. for each species and treatment, the mean activity, SD activity, and n

# I recognise this is not the tidyest way to do this, however it matches the format of the metadata data set

Clark_summary <- bind_cols(
  unique(Clark_clean %>% group_by(species) %>% filter(treatment == "control") 
         %>% mutate(ctrl.mean = mean(activity)) 
         %>% mutate(ctrl.sd = sd(activity)) 
         %>% select(species, ctrl.mean, ctrl.sd) 
         %>% arrange(species)), 
  (ctrl.n = (Clark_clean %>% group_by(species) %>% filter(treatment == "control") 
         %>% arrange(species)
         %>% count())[,2]), 
  unique(Clark_clean %>% group_by(species) %>% filter(treatment == "CO2") 
         %>% mutate(oa.mean = mean(activity)) 
         %>% mutate(oa.sd = sd(activity)) 
         %>% select(species, oa.mean, oa.sd) 
         %>% arrange(species))[,2:3], 
  (oa.n = (Clark_clean %>% group_by(species) %>% filter(treatment == "CO2") 
         %>% arrange(species) 
         %>% count())[,2])
  )

# rename the n columns and species column, to match the broader metadata data set used later
Clark_summary <- Clark_summary %>% rename(ctrl.n = n...4, oa.n = n...7, Species = species)

# replace the abbreviated species names with their full scientific names, to match the broader metadata data set used later
# Species name: acantho = Acanthochromis polyacanthus; 
#               ambon = Pomacentrus amboinensis; 
#               chromis = Chromis atripectoralis; 
#               humbug = Dascyllus aruanus; 
#               lemon = Pomacentrus moluccensis	
#               whitedams = Dischistodus perspicillatus
levels(Clark_summary$Species) <- c('Acanthochromis polyacanthus', 'Pomacentrus amboinensis', 'Chromis atripectoralis', 'Dascyllus aruanus', 'Pomacentrus moluccensis', 'Dischistodus perspicillatus')

# convert the species factor back to a character, to match the broader metadata data set used later
# why did I make species a factor in the first place? Because it makes renaming it much cleaner, and to make it easier to find any possible spelling errors in the data.

Clark_summary <- mutate(Clark_summary, Species = as.character(Species))
```


### Q2
Through coding, merge the summary statistics generated from 1) with the metadata (i.e., clark_paper_data.csv) from Clark et al. (2020).
```{r}
# Load the Clark et al. (2020) metadata from the data folder into an object "Clark_meta"
path <- "./data/clark_paper_data.csv"
Clark_meta <- read_csv(path)
```

```{r}
# merge the summary statistics with the metadata
Clark_meta <- bind_cols(Clark_meta, Clark_summary)
```

### Q3
Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset (i.e., ocean_meta_data.csv).
```{r}
# Load the larger meta-analysis dataset from the data folder into an object "meta"
path <- "./data/ocean_meta_data.csv"
meta <- read_csv(path)
```

```{r}
# A few columns need to be turned into character variables
# Additionally, NA values in the "Cue/stimulus type" column should be replaced with '-' to match the rest of the data set.
Clark_meta <- Clark_meta %>% mutate(`Pub year IF` = as.character(`Pub year IF`)) %>% mutate(`2017 IF`= as.character(`2017 IF`)) %>% mutate(`Env cue/stimulus?` = as.character(`Env cue/stimulus?`)) %>% mutate(`Cue/stimulus type` = as.character(`Cue/stimulus type`)) %>% mutate(`Cue/stimulus type` = replace_na(`Cue/stimulus type`, "-"))

str(Clark_meta)

```

```{r}
# merge the combined summary statistics and metadata from Clark et al. (2020) into the larger meta-analysis dataset

meta <- bind_rows(meta, Clark_meta)
```

### Q4
Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function.
```{r}
# lnRR produces NaNs whenever the sign of ctrl.mean and oa.mean are different.
# Since these either break or are automatically excluded from future analyses, and there arent many rows like this, its easiest to just remove them now
lnrr <- meta %>% filter(ctrl.mean > 0 & oa.mean > 0 | ctrl.mean < 0 & oa.mean < 0)
lnrr <- escalc(measure="ROM", m1i = ctrl.mean, m2i = oa.mean, sd1i = ctrl.sd, sd2i = oa.sd, n1i = ctrl.n, n2i = oa.n, data=lnrr)

```

### Q5
Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.
```{r}
# Producing an rma.mv model using the effect size (yi) and sampling variance (vi) calculated above
# Authors and Study are invluded as random effects

# Because the ratio of largest to smallest sampling variance extremely large, the model has a very difficult time producing a result with default settings.
# By default, rel.tol = 1e-8. To make this model actually produce a result, I've had to increase it all the way to 1e-2
# In theory I should also be able to make the model run more iterations with iter.max, to maybe get a result without changing the tolerance,
#  but for some reason that's not actually making it run more iterations in practice.
MLMA <- metafor::rma.mv(yi ~ 1, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr,
                   control = list(rel.tol = 1e-2))
MLMA
```

### Q6
Written paragraph of the findings and what they mean which is supported with a figure.
```{r, predict_I2, fig.align='center', fig.cap= "Table 1: I2 analysis showing the breakdown of which factors accounted for in the meta-regression model explain what percentage of variation in the log response ratio."}
predict <- predict(MLMA)

## Calculate I2
i2_vals <- orchaRd::i2_ml(MLMA)
```
  
The overall meta-analytic mean was `r x=coef(MLMA); x`, with a 95% confidence interval of `r x=MLMA$ci.lb; x` to `r x=MLMA$ci.ub; x`, and a p-value of `r x=MLMA$pval; x`.  
From this, we can interpret that the overall effect size among all the studies in the meta-analysis is not significantly different from zero.  
The prediction interval was `r x=predict$ci.lb; x` to `r x=predict$ci.ub; x`, which once again tells us that the effect size is not expected to be significantly different from zero.  

In the I^2 analysis we can see that `r x=i2_vals[2]; x`% of the variation in the log response ratio is explained by a difference in the study, and `r x=i2_vals[3]; x`% is explained by a difference in observation. Together, this explains all the variation, which is consistent with the true effect size not being significantly different from zero.  


### Q7
Funnel plot for visually assessing the possibility of publication bias.
```{r funnel, echo=TRUE, fig.align='center', fig.cap= "Figre 1: Funnel plot depicting precision (1 / SE) against the log response ratio (lnRR)."}
# Due to the extreme variation in the size of sampling error (and subsequently, the inverse standard error),
#  I have log transformed the y axis to compress the y axis to a readable scale.
# In order to do this, it seemed to be necessary to use ggplot rather than metafor to create the plot.
# As a consequence, the plot cant be contour-enhanced.

ggplot(lnrr, aes(y = abs(log(1/sqrt(vi))), x = yi)) + geom_point() + geom_vline(aes(xintercept = 0)) +
    labs(y = "Precision (1/SE)", x = "log Reponse Ratio (lnRR)") + theme_bw()
```

### Q8
Time-lag plot assessing how effect sizes may or may not have changed through time.
```{r, time-lag, fig.align='center', fig.cap= "Figure 2: Time-lag plot depicting effect size (lnRR) against the year in which the paper was published. Point size is scaled according to precision, 1/sqrt(vi)"}
# 'Year..online.' is the year the study was published, on the x axis
# 'yi' in the log response ratio, on the y axis
# Points are scaled in size according to their inverse standard error, 1/sqrt(vi)
# A linear model trend line is also applied
ggplot(lnrr, aes(x=Year..online., y=yi, size=1/sqrt(vi))) + geom_point(alpha=0.5) + geom_smooth(method = "lm", show.legend = F) + labs(size="Precision (1/SE)") + ylab("Effect Size (lnRR)") + xlab("Year Published")
```

### Q9
Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
# centre the year on zero and remove the rows with an NA value since apparently they break the meta regression
lnrr_temp <-  na.omit(lnrr) %>% mutate(Year_c = mean(Year..online.) - Year..online.)

# As before, rel.tol needs to be greatly increased compared to the default setting for this model to run
metareg_time_c <- rma.mv(yi ~ 1 + Year_c, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr_temp,
                   control = list(rel.tol = 1e-1))
summary(metareg_time_c) 
```
test for residual heterogeneity tells us there is a lot of unexplained variation left over
Test of moderators tells us that the moderators (in this case, year) are not explaining a significant amount of variation



### Q10
Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases
```{r}
# I see we've been asked to use inverse sampling variance as a moderator.
# I have used sampling variance instead, because the model simply does not run with inverse sampling variance,
#  and sampling variance is what was used in week 10 to test for file-drawer bias.
# I can only hope that asking for inverse sampling variance was a mistake.

metareg_time_f <- rma.mv(yi ~ vi, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr_temp,
                   control = list(rel.tol = 1e-2))
summary(metareg_time_f) 
```


### Q11
We can see from the time-lag plot Fig. \@ref(fig:time-lag) that not only does effect size not seem to be decreasing with time, it actually appears to be increasing with time.  
The dominance of low-power studies also appears to be consistent through time. This is contrary to what would be expected if a decline effect were present. A meta-regression model with year as a moderating factor similarly did not find any evidence that the year of publication is not explaining a significant amount of variation, with a 95% confidence interval of `r x=metareg_time_c$ci.lb[2]; x` to `r x=metareg_time_c$ci.ub[2]; x`, and a p-value of `r x=metareg_time_c$pval[2]; x`.    

The funnel plot Fig. \@ref(fig:funnel) does not appear to show any noteworthy asymmetry, with the exception of two relatively high-power studies showing a positive effect. Thus is does not seem there is any file-drawer bias at play either. Similarly, a meta-regression model with sampling variance as a moderating factor similarly did not find any evidence that the year of publication is not explaining a significant amount of variation, with a 95% confidence interval of `r x=metareg_time_f$ci.lb[2]; x` to `r x=metareg_time_f$ci.ub[2]; x`, and a p-value of `r x=metareg_time_f$pval[2]; x`.    


### Q12
In this dataset, `r x=count(lnrr[lnrr$Average.n < 30,]); x` out of 801 studies used in the analysis had an average sample size of below 30. We may consider only studies with an average sample size of above 30, however the initial analysis already found that the log response ratio was not likely to be significantly different from zero, and this is unlikely to change.

```{r}
MLMA_new <- metafor::rma.mv(yi ~ 1, V = vi, 
                   method="REML",
                   random=list(~1|Authors,
                               ~1|Study), 
                   dfs = "contain",
                   test="t",
                   data=lnrr[lnrr$Average.n > 29,])
```
As expected, when rerunning the initial meta-analysis while considering only papers with an average sample size of 30 or more, there is still no significant difference from zero in the log response ratio, with a confidence interval of `r x=MLMA_new$ci.lb; x` to `r x=MLMA_new$ci.ub; x`.


Clement et. al. (2022) found a decline effect when performing a meta-analysis on a much smaller set of studies on the effects of ocean acidification on fish behavior. In this much larger data set, a decline effect does not appear to be present. Much as in this analysis, Clement et. al. (2022) also found that log response ratio was not likely to be significantly different from zero.



